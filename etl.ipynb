{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0a141bb8a1dcc5bdbaba3bf5e50ce87adc184476e34f7b1ab430ffb7fc6254064",
   "display_name": "Python 3.9.2 64-bit ('3.9.2')"
  },
  "metadata": {
   "interpreter": {
    "hash": "a141bb8a1dcc5bdbaba3bf5e50ce87adc184476e34f7b1ab430ffb7fc6254064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:3.1.1 pyspark-shell'\n",
    "\n",
    "def create_spark_session():\n",
    "    conf = SparkConf().setAppName('pyspark_aws').setMaster('local[*]')\n",
    "\n",
    "    sc=SparkContext(conf=conf)\n",
    "\n",
    "    hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "    hadoopConf.set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    hadoopConf.set('fs.s3a.access.key', config['AWS']['AWS_ACCESS_KEY_ID'])\n",
    "    hadoopConf.set('fs.s3a.secret.key', config['AWS']['AWS_SECRET_ACCESS_KEY'])\n",
    "\n",
    "    spark=SparkSession(sc)\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"./data/output/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.json(\"s3a://udacity-dend/song_data/A/A/A/*.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n|ARTC1LV1187B9A4858|        51.4536|Goldsmith's Colle...|        -0.01802|  The Bonzo Dog Band|301.40036|        1|SOAFBCP12A8C13CC7D|King Of Scurf (20...|1972|\n|ARA23XO1187B9AF18F|       40.57885|Carteret, New Jersey|       -74.21956|     The Smithereens|  192.522|        1|SOKTJDS12AF72A25E5|Drown In My Own T...|   0|\n|ARSVTNL1187B992A91|       51.50632|     London, England|        -0.12714|       Jonathan King|129.85424|        1|SOEKAZG12AB018837E|I'll Slap Your Fa...|2001|\n|AR73AIO1187B9AD57B|       37.77916|   San Francisco, CA|      -122.42005|   Western Addiction|118.07302|        1|SOQPWCR12A6D4FB2A3|A Poor Recipe For...|2005|\n|ARXQBR11187B98A2CC|           null|  Liverpool, England|            null|Frankie Goes To H...|821.05424|        1|SOBRKGM12A8C139EF6|Welcome to the Pl...|1985|\n+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- artist_id: string (nullable = true)\n |-- artist_latitude: double (nullable = true)\n |-- artist_location: string (nullable = true)\n |-- artist_longitude: double (nullable = true)\n |-- artist_name: string (nullable = true)\n |-- duration: double (nullable = true)\n |-- num_songs: long (nullable = true)\n |-- song_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- year: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table = df[\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['artist_id',\n",
       " 'artist_name',\n",
       " 'artist_location',\n",
       " 'artist_latitude',\n",
       " 'artist_longitude']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "songs_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata = input_data + \"song_data/A/A/A/*.json\"\n",
    "\n",
    "# read song data file\n",
    "df = spark.read.json(songdata)\n",
    "df.createOrReplaceTempView(\"songdata_view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "songs_table = spark.sql(\"\"\"SELECT DISTINCT song_id, title, artist_id, year, duration\n",
    "                                FROM songdata_view\n",
    "                            WHERE song_id IS NOT NULL \n",
    "                    \"\"\")\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.mode(\"overwrite\").partitionBy(\"year\",\"artist_id\").parquet(output_data+\"songs_table/\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "artists_table = spark.sql(\"\"\"SELECT DISTINCT \n",
    "                                        artist_id, artist_name as name, artist_location as location, \n",
    "                                        artist_latitude as lattitude, artist_longitude as longitude\n",
    "                                   FROM songdata_view\n",
    "                                  WHERE artist_id IS NOT NULL \n",
    "                              \"\"\")\n",
    "    \n",
    "# write artists table to parquet files\n",
    "artists_table.write.mode(\"overwrite\").parquet(output_data+\"artists_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = input_data + \"log_data/*/*/*.json\"\n",
    "\n",
    "# read log data file\n",
    "df = spark.read.json(log_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filter by actions for song plays\n",
    "df.filter(df.page=='NextSong').createOrReplaceTempView(\"log_data_view\")\n",
    "\n",
    "# extract columns for users table    \n",
    "users_table = spark.sql(\"\"\"SELECT DISTINCT userId as user_id, firstName as first_name, \n",
    "                                      lastName as last_name, gender, level\n",
    "                                FROM log_data_view\n",
    "                            WHERE userId IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.mode(\"overwrite\").parquet(output_data+\"users_table/\")\n",
    "\n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(int(int(x)/1000)), TimestampType())\n",
    "df = df.withColumn('datetime', get_datetime(df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: long (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: double (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: double (nullable = true)\n |-- sessionId: long (nullable = true)\n |-- song: string (nullable = true)\n |-- status: long (nullable = true)\n |-- ts: long (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)\n |-- start_time: string (nullable = true)\n |-- datetime: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+----+---+----+-----+----+-------+\n|         start_time|hour|day|week|month|year|weekday|\n+-------------------+----+---+----+-----+----+-------+\n|2018-11-21 11:56:46|  11| 21|  47|   11|2018|      4|\n|2018-11-21 19:49:29|  19| 21|  47|   11|2018|      4|\n|2018-11-14 10:26:16|  10| 14|  46|   11|2018|      4|\n|2018-11-14 11:30:29|  11| 14|  46|   11|2018|      4|\n|2018-11-28 02:44:16|   2| 28|  48|   11|2018|      4|\n+-------------------+----+---+----+-----+----+-------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# extract columns to create time table\n",
    "df.createOrReplaceTempView(\"log_data_view\")\n",
    "time_table = spark.sql(\"\"\"SELECT DISTINCT\n",
    "                                    start_time, \n",
    "                                    hour(datetime) as hour, \n",
    "                                    day(datetime) as day, \n",
    "                                    weekofyear(datetime) as week, \n",
    "                                    month(datetime) as month, \n",
    "                                    year(datetime) as year, \n",
    "                                    dayofweek(datetime) as weekday\n",
    "                            FROM log_data_view\n",
    "                            WHERE datetime IS NOT NULL  \n",
    "\"\"\")\n",
    "    \n",
    "time_table.show(5)\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(output_data+\"time_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.parquet(output_data+\"songs_table/\")\n",
    "song_df.createOrReplaceTempView(\"songs_table\")\n",
    "\n",
    "artist_df = spark.read.parquet(output_data+\"artists_table/\")\n",
    "artist_df.createOrReplaceTempView(\"artists_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "# songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "songplays_table = spark.sql(\"\"\"SELECT monotonically_increasing_id() as songplay_id,\n",
    "                                        l.start_time, year(l.datetime) as year, month(l.datetime) as month, \n",
    "                                        l.userId, l.level, l.song, s.song_id, l.artist, a.artist_id, \n",
    "                                        l.sessionId, l.location, l.userAgent\n",
    "                                    FROM log_data_view l\n",
    "                            LEFT JOIN songs_table s ON (s.title = l.song)\n",
    "                            LEFT JOIN artists_table a ON (a.name = l.artist)   \n",
    "                            \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+-------------------+----+-----+------+-----+---------------+-------+-----------+---------+---------+--------------------+--------------------+\n|songplay_id|         start_time|year|month|userId|level|           song|song_id|     artist|artist_id|sessionId|            location|           userAgent|\n+-----------+-------------------+----+-----+------+-----+---------------+-------+-----------+---------+---------+--------------------+--------------------+\n|          0|2018-11-15 01:30:26|2018|   11|    26| free|  Sehr kosmisch|   null|   Harmonia|     null|      583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n|          1|2018-11-15 01:41:21|2018|   11|    26| free|The Big Gundown|   null|The Prodigy|     null|      583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n|          2|2018-11-15 01:45:41|2018|   11|    26| free|       Marry Me|   null|      Train|     null|      583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n|          3|2018-11-15 02:57:51|2018|   11|     9| free|           null|   null|       null|     null|      563|Eureka-Arcata-For...|Mozilla/5.0 (Wind...|\n|          4|2018-11-15 04:29:37|2018|   11|    12| free|           null|   null|       null|     null|      521|New York-Newark-J...|Mozilla/5.0 (Wind...|\n+-----------+-------------------+----+-----+------+-----+---------------+-------+-----------+---------+---------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "songplays_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(output_data+\"songplays_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}